{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7822867",
   "metadata": {},
   "source": [
    "# 1. Road Network Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bfdb1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRS: EPSG:3395\n",
      "Rows: 14769\n",
      "Columns: ['Name', 'Route_Type', 'Type', 'Lower_Date', 'Low_Date_E', 'Upper_Date', 'Up_Date_E', 'Descriptio', 'Citation', 'Bibliograp', 'Cons_per_e', 'Itinerary', 'Segment_s', 'Avg_Slope', 'passabilit', 'Shape_Leng', 'InLine_FID', 'MaxSimpTol', 'MinSimpTol', 'geometry']\n",
      "Geometry types:\n",
      " MultiLineString    14769\n",
      "Name: count, dtype: int64\n",
      "Total bounds: [-1051606.9533      2578001.81270617  4627965.33944631  7518329.0997    ]\n",
      "After explode:\n",
      "CRS: EPSG:3395\n",
      "Rows: 14769\n",
      "Geometry types:\n",
      " LineString    14769\n",
      "Name: count, dtype: int64\n",
      "Empty geometries: 0\n",
      "Null geometries: 0\n",
      "unary_union result geom_type: MultiLineString\n",
      "Noded edges:\n",
      "Rows: 16854\n",
      "Geometry types:\n",
      " LineString    16854\n",
      "Name: count, dtype: int64\n",
      "Empty: 0\n",
      "Null: 0\n",
      "Input total length: 400175337.99454165\n",
      "Noded total length: 399914306.6043441\n",
      "Nodes (unique endpoints): 13860\n",
      "Degree summary:\n",
      "count    13860.000000\n",
      "mean         2.432035\n",
      "std          1.034891\n",
      "min          1.000000\n",
      "25%          2.000000\n",
      "50%          3.000000\n",
      "75%          3.000000\n",
      "max         10.000000\n",
      "Name: degree, dtype: float64\n",
      "Degree counts (top 10):\n",
      "degree\n",
      "3     5040\n",
      "2     3628\n",
      "1     3217\n",
      "4     1827\n",
      "5      103\n",
      "6       30\n",
      "7       10\n",
      "8        4\n",
      "10       1\n",
      "Name: count, dtype: int64\n",
      "Edges: 16854\n",
      "Sum of degrees: 33708\n",
      "Detailed graph: 998,152 nodes, 1,001,666 edges\n",
      "Projected simplified edges: 12874\n",
      "Projected simplified nodes: 9624\n",
      "H1 (deg-2 removed): 9,624 nodes, 12,874 edges\n",
      "Roundabout merge candidates: 7022\n",
      "Nodes merged (before): 295\n",
      "Centroids (after): 136\n",
      "{'roads_before': 14769, 'roads_after': 16854, 'deg2_removed': 988210, 'deg2_remain': 9624, 'round_members': 295, 'round_centroids': 136, 'snap_members': 216, 'snap_centroids': 97, 'noding_created': 729, 'noding_remain': 11146, 'noding_junctions': 7808}\n",
      "Saved: full_cleaning_nodes_before_after.html\n"
     ]
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import Point, LineString\n",
    "from sklearn.cluster import DBSCAN\n",
    "import folium\n",
    "\n",
    "# 1) Load\n",
    "gdf = gpd.read_file(\"17122148/itinere_roads.geojson\")\n",
    "\n",
    "# 2) Inspect CRS / projection\n",
    "print(\"CRS:\", gdf.crs)\n",
    "print(\"Rows:\", len(gdf))\n",
    "print(\"Columns:\", list(gdf.columns))\n",
    "print(\"Geometry types:\\n\", gdf.geometry.geom_type.value_counts(dropna=False))\n",
    "\n",
    "# 3) Show a quick bounds check (helps detect wrong CRS)\n",
    "print(\"Total bounds:\", gdf.total_bounds)  # [minx, miny, maxx, maxy]\n",
    "\n",
    "# 4) Decide projected vs geographic versions\n",
    "# If the CRS is missing, assume EPSG:4326 (WGS84 lon/lat) and set it.\n",
    "if gdf.crs is None:\n",
    "    gdf = gdf.set_crs(epsg=4326, allow_override=True)\n",
    "    print(\"CRS was missing -> set to EPSG:4326\")\n",
    "\n",
    "# Create two standard views:\n",
    "# - gdf_ll: geographic (EPSG:4326)\n",
    "# - gdf_m: metric/projected (EPSG:3395) for distance-based operations\n",
    "gdf_ll = gdf.to_crs(epsg=4326)\n",
    "gdf_m = gdf.to_crs(epsg=3395)\n",
    "\n",
    "# Starting from your already-loaded gdf (EPSG:3395)\n",
    "gdf_m = gdf  # keep naming consistent: metric CRS\n",
    "\n",
    "# Explode MultiLineStrings into LineStrings\n",
    "gdf_lines = gdf_m.explode(index_parts=True).reset_index(drop=True)\n",
    "\n",
    "print(\"After explode:\")\n",
    "print(\"CRS:\", gdf_lines.crs)\n",
    "print(\"Rows:\", len(gdf_lines))\n",
    "print(\"Geometry types:\\n\", gdf_lines.geometry.geom_type.value_counts(dropna=False))\n",
    "\n",
    "# Quick check: any empties?\n",
    "print(\"Empty geometries:\", int(gdf_lines.geometry.is_empty.sum()))\n",
    "print(\"Null geometries:\", int(gdf_lines.geometry.isna().sum()))\n",
    "\n",
    "# 1) Node + split at intersections + overlaps\n",
    "u = unary_union(list(gdf_lines.geometry))\n",
    "\n",
    "print(\"unary_union result geom_type:\", u.geom_type)\n",
    "\n",
    "# 2) Extract LineStrings into a list\n",
    "lines = []\n",
    "if u.geom_type == \"LineString\":\n",
    "    lines = [u]\n",
    "elif u.geom_type == \"MultiLineString\":\n",
    "    lines = list(u.geoms)\n",
    "else:\n",
    "    # GeometryCollection or other: collect any LineStrings/MultiLineStrings inside\n",
    "    for gg in getattr(u, \"geoms\", []):\n",
    "        if gg.geom_type == \"LineString\":\n",
    "            lines.append(gg)\n",
    "        elif gg.geom_type == \"MultiLineString\":\n",
    "            lines.extend(list(gg.geoms))\n",
    "\n",
    "# 3) Make edges GeoDataFrame\n",
    "edges_noded = gpd.GeoDataFrame({\"geometry\": lines}, crs=gdf_lines.crs)\n",
    "\n",
    "print(\"Noded edges:\")\n",
    "print(\"Rows:\", len(edges_noded))\n",
    "print(\"Geometry types:\\n\", edges_noded.geometry.geom_type.value_counts(dropna=False))\n",
    "\n",
    "# 4) Quick sanity checks\n",
    "print(\"Empty:\", int(edges_noded.geometry.is_empty.sum()))\n",
    "print(\"Null:\", int(edges_noded.geometry.isna().sum()))\n",
    "\n",
    "# 5) Optional: total length comparison (should be close-ish, can change due to overlap handling)\n",
    "print(\"Input total length:\", float(gdf_lines.length.sum()))\n",
    "print(\"Noded total length:\", float(edges_noded.length.sum()))\n",
    "\n",
    "# edges_noded from step 3 is now a GeoDataFrame of LineStrings \n",
    "# representing the noded edges. \n",
    "# Next, we want to build a nodes GeoDataFrame and map edges \n",
    "# to node IDs.\n",
    "# Build node candidates from every edge endpoint\n",
    "start_pts = edges_noded.geometry.apply(lambda ls: Point(ls.coords[0]))\n",
    "end_pts   = edges_noded.geometry.apply(lambda ls: Point(ls.coords[-1]))\n",
    "\n",
    "nodes_raw = gpd.GeoDataFrame(\n",
    "    {\"geometry\": pd.concat([start_pts, end_pts], ignore_index=True)},\n",
    "    crs=edges_noded.crs,\n",
    ")\n",
    "\n",
    "# Deduplicate nodes (exact coordinate match)\n",
    "nodes_raw[\"wkb\"] = nodes_raw.geometry.apply(lambda p: p.wkb_hex)\n",
    "nodes = nodes_raw.drop_duplicates(\"wkb\").drop(columns=\"wkb\").reset_index(drop=True)\n",
    "nodes[\"node_id\"] = nodes.index.astype(\"int64\") + 1\n",
    "\n",
    "print(\"Nodes (unique endpoints):\", len(nodes))\n",
    "\n",
    "# Map endpoints -> node_id\n",
    "node_map = {p.wkb_hex: nid for p, nid in zip(nodes.geometry, nodes[\"node_id\"])}\n",
    "\n",
    "from_ids = []\n",
    "to_ids = []\n",
    "for ls in edges_noded.geometry:\n",
    "    a = Point(ls.coords[0]).wkb_hex\n",
    "    b = Point(ls.coords[-1]).wkb_hex\n",
    "    from_ids.append(node_map[a])\n",
    "    to_ids.append(node_map[b])\n",
    "\n",
    "edges = edges_noded.copy()\n",
    "edges[\"edge_id\"] = edges.index.astype(\"int64\") + 1\n",
    "edges[\"from_node\"] = from_ids\n",
    "edges[\"to_node\"] = to_ids\n",
    "\n",
    "# 4) Compute node degree from edges\n",
    "deg = pd.concat([edges[\"from_node\"], edges[\"to_node\"]]).value_counts()\n",
    "nodes[\"degree\"] = nodes[\"node_id\"].map(deg).fillna(0).astype(int)\n",
    "\n",
    "print(\"Degree summary:\")\n",
    "print(nodes[\"degree\"].describe())\n",
    "\n",
    "print(\"Degree counts (top 10):\")\n",
    "print(nodes[\"degree\"].value_counts().head(10))\n",
    "\n",
    "# 5) Sanity check: edges count and implied degree sum\n",
    "print(\"Edges:\", len(edges))\n",
    "print(\"Sum of degrees:\", int(nodes[\"degree\"].sum()))\n",
    "\n",
    "\n",
    "def simplify_degree2(G: nx.Graph):\n",
    "    \"\"\"\n",
    "    Collapse degree-2 chains into single edges.\n",
    "    Keeps endpoints (deg==1) and junctions (deg>=3) as 'important' nodes.\n",
    "    Returns:\n",
    "      H: simplified graph\n",
    "      edge_geoms: list of dicts with true polyline geometry\n",
    "                 [{\"u\":u, \"v\":v, \"weight\":w, \"geometry\":LineString([...])}, ...]\n",
    "    \"\"\"\n",
    "    # nodes to keep\n",
    "    important = {n for n, deg in G.degree() if deg != 2}\n",
    "\n",
    "    H = nx.Graph()\n",
    "    edge_geoms = []\n",
    "\n",
    "    # Track which directed adjacency traversals we already used\n",
    "    seen_dir = set()\n",
    "\n",
    "    for u in important:\n",
    "        for v in G.neighbors(u):\n",
    "            if (u, v) in seen_dir:\n",
    "                continue\n",
    "\n",
    "            # start path u -> v\n",
    "            coords = [u, v]\n",
    "            total_w = float(G[u][v].get(\"weight\", 0.0))\n",
    "\n",
    "            seen_dir.add((u, v))\n",
    "            seen_dir.add((v, u))\n",
    "\n",
    "            prev = u\n",
    "            cur = v\n",
    "\n",
    "            # walk through degree-2 nodes until we reach another important node\n",
    "            while cur not in important:\n",
    "                nbrs = [n for n in G.neighbors(cur) if n != prev]\n",
    "                if len(nbrs) != 1:\n",
    "                    break\n",
    "                nxt = nbrs[0]\n",
    "                total_w += float(G[cur][nxt].get(\"weight\", 0.0))\n",
    "\n",
    "                prev, cur = cur, nxt\n",
    "                coords.append(cur)\n",
    "\n",
    "                seen_dir.add((prev, cur))\n",
    "                seen_dir.add((cur, prev))\n",
    "\n",
    "            if cur == u:\n",
    "                continue\n",
    "\n",
    "            # add/merge edge in simplified graph\n",
    "            if H.has_edge(u, cur):\n",
    "                H[u][cur][\"weight\"] = min(float(H[u][cur].get(\"weight\", total_w)), total_w)\n",
    "            else:\n",
    "                H.add_edge(u, cur, weight=total_w)\n",
    "\n",
    "            edge_geoms.append({\n",
    "                \"u\": u,\n",
    "                \"v\": cur,\n",
    "                \"weight\": float(total_w),\n",
    "                \"geometry\": LineString(coords),\n",
    "            })\n",
    "\n",
    "    # Ensure isolated important nodes are kept\n",
    "    for n in important:\n",
    "        if n not in H:\n",
    "            H.add_node(n)\n",
    "\n",
    "    return H, edge_geoms\n",
    "\n",
    "def simplify_degree2_until_stable(G: nx.Graph, max_iter: int = 10):\n",
    "    \"\"\"\n",
    "    Repeat degree-2 simplification until no further changes.\n",
    "    Useful because after simplification, some previously 'important' nodes can become degree-2.\n",
    "    Returns:\n",
    "      H: final simplified graph\n",
    "      edge_geoms: geometries for the last pass (true polylines)\n",
    "    \"\"\"\n",
    "    cur = G\n",
    "    last_geoms = []\n",
    "\n",
    "    for _ in range(int(max_iter)):\n",
    "        nxt, geoms = simplify_degree2(cur)\n",
    "\n",
    "        if (nxt.number_of_nodes() == cur.number_of_nodes()) and (nxt.number_of_edges() == cur.number_of_edges()):\n",
    "            return nxt, geoms\n",
    "\n",
    "        cur = nxt\n",
    "        last_geoms = geoms\n",
    "\n",
    "    return cur, last_geoms\n",
    "\n",
    "\n",
    "# REQUIRED: You should already have simplify_degree2_until_stable from earlier work.\n",
    "# It must return: (H1, edge_geoms1) where edge_geoms1 has {\"u\",\"v\",\"weight\",\"geometry\"} with true polylines.\n",
    "def build_detailed_graph_from_edges(edges_gdf) -> nx.Graph:\n",
    "    G = nx.Graph()\n",
    "    for line in edges_gdf.geometry:\n",
    "        coords = list(line.coords)\n",
    "        if len(coords) < 2:\n",
    "            continue\n",
    "        for (x1, y1, *_), (x2, y2, *__) in zip(coords[:-1], coords[1:]):\n",
    "            u = (float(x1), float(y1))\n",
    "            v = (float(x2), float(y2))\n",
    "            if u == v:\n",
    "                continue\n",
    "            w = float(np.hypot(v[0] - u[0], v[1] - u[1]))\n",
    "            if w <= 0:\n",
    "                continue\n",
    "            if G.has_edge(u, v):\n",
    "                # keep the shortest if duplicates exist\n",
    "                G[u][v][\"weight\"] = min(G[u][v][\"weight\"], w)\n",
    "            else:\n",
    "                G.add_edge(u, v, weight=w)\n",
    "    return G\n",
    "\n",
    "G = build_detailed_graph_from_edges(edges_noded)\n",
    "print(f\"Detailed graph: {G.number_of_nodes():,} nodes, {G.number_of_edges():,} edges\")\n",
    "H1, edge_geoms1 = simplify_degree2_until_stable(G)\n",
    "\n",
    "# 1) Edges GeoDataFrame (projected)\n",
    "edges_simpl_3395 = gpd.GeoDataFrame(edge_geoms1, crs=edges_noded.crs).copy()\n",
    "edges_simpl_3395 = edges_simpl_3395.drop_duplicates(subset=[\"u\", \"v\", \"weight\"]).reset_index(drop=True)\n",
    "edges_simpl_3395[\"edge_id\"] = edges_simpl_3395.index.astype(\"int64\") + 1\n",
    "\n",
    "# 2) Nodes GeoDataFrame (projected)\n",
    "nodes_list = list(H1.nodes())\n",
    "nodes_simpl_3395 = gpd.GeoDataFrame(\n",
    "    {\n",
    "        \"node_id\": np.arange(1, len(nodes_list) + 1, dtype=\"int64\"),\n",
    "        \"degree\": [H1.degree(n) for n in nodes_list],\n",
    "        \"x\": [n[0] for n in nodes_list],\n",
    "        \"y\": [n[1] for n in nodes_list],\n",
    "    },\n",
    "    geometry=gpd.points_from_xy([n[0] for n in nodes_list], [n[1] for n in nodes_list]),\n",
    "    crs=edges_noded.crs,\n",
    ")\n",
    "\n",
    "# 3) Add from/to node IDs to edges (stable mapping)\n",
    "node_id_map = {n: i+1 for i, n in enumerate(nodes_list)}\n",
    "edges_simpl_3395[\"from_node\"] = edges_simpl_3395[\"u\"].map(node_id_map).astype(\"int64\")\n",
    "edges_simpl_3395[\"to_node\"]   = edges_simpl_3395[\"v\"].map(node_id_map).astype(\"int64\")\n",
    "\n",
    "print(\"Projected simplified edges:\", len(edges_simpl_3395))\n",
    "print(\"Projected simplified nodes:\", len(nodes_simpl_3395))\n",
    "\n",
    "# 4) Create unprojected versions (EPSG:4326)\n",
    "nodes_simpl_4326 = nodes_simpl_3395.to_crs(epsg=4326)\n",
    "edges_simpl_4326 = edges_simpl_3395.to_crs(epsg=4326)\n",
    "\n",
    "H1, edge_geoms1 = simplify_degree2_until_stable(G)\n",
    "print(f\"H1 (deg-2 removed): {H1.number_of_nodes():,} nodes, {H1.number_of_edges():,} edges\")\n",
    "\n",
    "def merge_close_intersections_dbscan(H: nx.Graph, radius_m: float = 200.0, min_degree: int = 3):\n",
    "    \"\"\"\n",
    "    Merge nodes with degree>=min_degree within radius_m into centroid nodes.\n",
    "    Returns:\n",
    "      Hm: merged graph\n",
    "      mapping: old_node -> new_node\n",
    "      cluster_info: dict with diagnostic info for plotting\n",
    "    \"\"\"\n",
    "    candidates = [n for n in H.nodes() if H.degree(n) >= min_degree]\n",
    "    if len(candidates) == 0:\n",
    "        return H.copy(), {n: n for n in H.nodes()}, {\n",
    "            \"candidates\": [],\n",
    "            \"labels\": np.array([], dtype=int),\n",
    "            \"centroids\": {},\n",
    "            \"merge_nodes_before\": set(),\n",
    "            \"centroid_nodes_after\": set(),\n",
    "        }\n",
    "\n",
    "    X = np.asarray(candidates, dtype=float)  # (k,2)\n",
    "    db = DBSCAN(eps=radius_m, min_samples=1).fit(X)\n",
    "    labels = db.labels_\n",
    "\n",
    "    # Centroid per cluster label\n",
    "    centroids = {}\n",
    "    for lab in np.unique(labels):\n",
    "        pts = X[labels == lab]\n",
    "        centroids[int(lab)] = (float(pts[:, 0].mean()), float(pts[:, 1].mean()))\n",
    "\n",
    "    # Nodes that actually merge = clusters with size > 1\n",
    "    merge_nodes_before = set()\n",
    "    centroid_nodes_after = set()\n",
    "    for lab in np.unique(labels):\n",
    "        members = [candidates[i] for i in np.where(labels == lab)[0]]\n",
    "        if len(members) > 1:\n",
    "            merge_nodes_before.update(members)\n",
    "            centroid_nodes_after.add(centroids[int(lab)])\n",
    "\n",
    "    # Mapping old -> new (only candidates move)\n",
    "    mapping = {n: n for n in H.nodes()}\n",
    "    for n, lab in zip(candidates, labels):\n",
    "        mapping[n] = centroids[int(lab)]\n",
    "\n",
    "    # Build merged graph\n",
    "    Hm = nx.Graph()\n",
    "    for u, v, d in H.edges(data=True):\n",
    "        u2 = mapping[u]\n",
    "        v2 = mapping[v]\n",
    "        if u2 == v2:\n",
    "            continue\n",
    "        w = float(d.get(\"weight\", 1.0))\n",
    "        if Hm.has_edge(u2, v2):\n",
    "            Hm[u2][v2][\"weight\"] = min(Hm[u2][v2].get(\"weight\", w), w)\n",
    "        else:\n",
    "            Hm.add_edge(u2, v2, weight=w)\n",
    "\n",
    "    # Keep isolated nodes\n",
    "    for n2 in set(mapping.values()):\n",
    "        if n2 not in Hm:\n",
    "            Hm.add_node(n2)\n",
    "\n",
    "    cluster_info = {\n",
    "        \"candidates\": candidates,\n",
    "        \"labels\": labels,\n",
    "        \"centroids\": centroids,\n",
    "        \"merge_nodes_before\": merge_nodes_before,\n",
    "        \"centroid_nodes_after\": centroid_nodes_after,\n",
    "    }\n",
    "    return Hm, mapping, cluster_info\n",
    "\n",
    "\n",
    "# Optional: merge roundabouts (topology) to produce an \"info\" dict for mapping\n",
    "H2, mapping, info_roundabout = merge_close_intersections_dbscan(H1, radius_m=200.0, min_degree=3)\n",
    "print(\"Roundabout merge candidates:\", len(info_roundabout.get(\"candidates\", [])))\n",
    "print(\"Nodes merged (before):\", len(info_roundabout.get(\"merge_nodes_before\", [])))\n",
    "print(\"Centroids (after):\", len(info_roundabout.get(\"centroid_nodes_after\", [])))\n",
    "\n",
    "# ----------------------------\n",
    "# utilities\n",
    "# ----------------------------\n",
    "def clip_to_region(gdf, region_poly):\n",
    "    minx, miny, maxx, maxy = region_poly.bounds\n",
    "    out = gdf.cx[minx:maxx, miny:maxy]\n",
    "    return out[out.intersects(region_poly)].copy()\n",
    "\n",
    "def key_xy(xy, prec_m=0.01):\n",
    "    return (round(float(xy[0]) / prec_m) * prec_m, round(float(xy[1]) / prec_m) * prec_m)\n",
    "\n",
    "def endpoints_keys_and_degree(lines_gdf, prec_m=0.01):\n",
    "    deg = {}\n",
    "    keys = set()\n",
    "    for ls in lines_gdf.geometry:\n",
    "        c = list(ls.coords)\n",
    "        a = key_xy(c[0], prec_m)\n",
    "        b = key_xy(c[-1], prec_m)\n",
    "        keys.add(a); keys.add(b)\n",
    "        deg[a] = deg.get(a, 0) + 1\n",
    "        deg[b] = deg.get(b, 0) + 1\n",
    "    return keys, deg\n",
    "\n",
    "def keys_to_points_gdf(keys, crs=\"EPSG:3395\"):\n",
    "    pts = [Point(x, y) for (x, y) in keys]\n",
    "    return gpd.GeoDataFrame({\"n\": np.arange(len(pts))}, geometry=pts, crs=crs)\n",
    "\n",
    "def sample_points_gdf(gdf, max_points, seed=0):\n",
    "    if max_points is None or len(gdf) <= max_points:\n",
    "        return gdf\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.choice(len(gdf), size=int(max_points), replace=False)\n",
    "    return gdf.iloc[idx].copy()\n",
    "\n",
    "# ----------------------------\n",
    "# layers for the 4 operations\n",
    "# ----------------------------\n",
    "def nodes_removed_degree2(G, H1, region_poly=None):\n",
    "    H1_nodes = set(H1.nodes())\n",
    "    deg = dict(G.degree())\n",
    "    removed = []\n",
    "    for n, d in deg.items():\n",
    "        if d == 2 and n not in H1_nodes:\n",
    "            if region_poly is None or region_poly.contains(Point(n)):\n",
    "                removed.append(n)\n",
    "    return gpd.GeoDataFrame(geometry=[Point(xy) for xy in removed], crs=\"EPSG:3395\")\n",
    "\n",
    "def nodes_remain_after_degree2(H1, region_poly=None):\n",
    "    nodes = [n for n in H1.nodes() if region_poly is None or region_poly.contains(Point(n))]\n",
    "    return gpd.GeoDataFrame(geometry=[Point(xy) for xy in nodes], crs=\"EPSG:3395\")\n",
    "\n",
    "def nodes_roundabout_layers(info_roundabout, region_poly=None):\n",
    "    if info_roundabout is None:\n",
    "        empty = gpd.GeoDataFrame(geometry=[], crs=\"EPSG:3395\")\n",
    "        return empty, empty\n",
    "\n",
    "    members = list(info_roundabout.get(\"merge_nodes_before\", []))\n",
    "    cents   = list(info_roundabout.get(\"centroid_nodes_after\", []))\n",
    "\n",
    "    if region_poly is not None:\n",
    "        members = [xy for xy in members if region_poly.contains(Point(xy))]\n",
    "        cents   = [xy for xy in cents if region_poly.contains(Point(xy))]\n",
    "\n",
    "    g_members = gpd.GeoDataFrame(geometry=[Point(xy) for xy in members], crs=\"EPSG:3395\")\n",
    "    g_cents   = gpd.GeoDataFrame(geometry=[Point(xy) for xy in cents], crs=\"EPSG:3395\")\n",
    "    return g_members, g_cents\n",
    "\n",
    "def snap_clusters_from_graph(H1, snap_radius_m=30.0, min_degree=3, region_poly=None):\n",
    "    nodes = [n for n in H1.nodes() if H1.degree(n) >= min_degree]\n",
    "    if region_poly is not None:\n",
    "        nodes = [n for n in nodes if region_poly.contains(Point(n))]\n",
    "\n",
    "    if len(nodes) < 2:\n",
    "        empty = gpd.GeoDataFrame(geometry=[], crs=\"EPSG:3395\")\n",
    "        return empty, empty\n",
    "\n",
    "    X = np.asarray(nodes, dtype=float)\n",
    "    db = DBSCAN(eps=float(snap_radius_m), min_samples=2).fit(X)  # only true clusters\n",
    "    labels = db.labels_\n",
    "\n",
    "    clusters = {}\n",
    "    for xy, lab in zip(nodes, labels):\n",
    "        if lab == -1:\n",
    "            continue\n",
    "        clusters.setdefault(int(lab), []).append(xy)\n",
    "\n",
    "    members = []\n",
    "    centroids = []\n",
    "    for mem in clusters.values():\n",
    "        xs = [m[0] for m in mem]\n",
    "        ys = [m[1] for m in mem]\n",
    "        members.extend(mem)\n",
    "        centroids.append((float(np.mean(xs)), float(np.mean(ys))))\n",
    "\n",
    "    g_members = gpd.GeoDataFrame(geometry=[Point(xy) for xy in members], crs=\"EPSG:3395\")\n",
    "    g_cents   = gpd.GeoDataFrame(geometry=[Point(xy) for xy in centroids], crs=\"EPSG:3395\")\n",
    "    return g_members, g_cents\n",
    "\n",
    "def noding_created_and_remaining_nodes(roads_before, roads_after, prec_m=0.01, min_junction_deg=3):\n",
    "    before_keys, _ = endpoints_keys_and_degree(roads_before, prec_m=prec_m)\n",
    "    after_keys, after_deg = endpoints_keys_and_degree(roads_after, prec_m=prec_m)\n",
    "\n",
    "    created = after_keys - before_keys\n",
    "    created_gdf = keys_to_points_gdf(created)\n",
    "\n",
    "    remain_gdf = keys_to_points_gdf(after_keys)\n",
    "\n",
    "    junction_keys = {k for k, d in after_deg.items() if d >= min_junction_deg}\n",
    "    junction_gdf = keys_to_points_gdf(junction_keys)\n",
    "\n",
    "    return created_gdf, remain_gdf, junction_gdf\n",
    "\n",
    "# ----------------------------\n",
    "# main: build layers + summary\n",
    "# ----------------------------\n",
    "def build_cleaning_layers(\n",
    "    gdf_lines_3395: gpd.GeoDataFrame,\n",
    "    edges_noded_3395: gpd.GeoDataFrame,\n",
    "    G_detailed,\n",
    "    H1_deg2_removed,\n",
    "    info_roundabout=None,\n",
    "    region_poly=None,\n",
    "    snap_radius_m=30.0,\n",
    "    min_junction_deg=3,\n",
    "    endpoint_prec_m=0.01,\n",
    "    max_points_removed=15000,\n",
    "    max_points_remain=25000,\n",
    "    seed=0\n",
    "):\n",
    "    roads_before = clip_to_region(gdf_lines_3395, region_poly) if region_poly is not None else gdf_lines_3395.copy()\n",
    "    roads_after  = clip_to_region(edges_noded_3395, region_poly) if region_poly is not None else edges_noded_3395.copy()\n",
    "\n",
    "    g_deg2_removed = nodes_removed_degree2(G_detailed, H1_deg2_removed, region_poly)\n",
    "    g_deg2_remain  = nodes_remain_after_degree2(H1_deg2_removed, region_poly)\n",
    "\n",
    "    g_deg2_removed = sample_points_gdf(g_deg2_removed, max_points_removed, seed=seed)\n",
    "    g_deg2_remain  = sample_points_gdf(g_deg2_remain,  max_points_remain,  seed=seed+1)\n",
    "\n",
    "    g_round_members, g_round_centroids = nodes_roundabout_layers(info_roundabout, region_poly)\n",
    "\n",
    "    g_snap_members, g_snap_centroids = snap_clusters_from_graph(\n",
    "        H1_deg2_removed,\n",
    "        snap_radius_m=snap_radius_m,\n",
    "        min_degree=min_junction_deg,\n",
    "        region_poly=region_poly,\n",
    "    )\n",
    "\n",
    "    g_noded_created, g_noded_remain, g_noded_junctions = noding_created_and_remaining_nodes(\n",
    "        roads_before, roads_after,\n",
    "        prec_m=endpoint_prec_m,\n",
    "        min_junction_deg=min_junction_deg,\n",
    "    )\n",
    "    g_noded_created   = sample_points_gdf(g_noded_created,   max_points_removed, seed=seed+2)\n",
    "    g_noded_remain    = sample_points_gdf(g_noded_remain,    max_points_remain,  seed=seed+3)\n",
    "    g_noded_junctions = sample_points_gdf(g_noded_junctions, max_points_remain,  seed=seed+4)\n",
    "\n",
    "    layers = {\n",
    "        \"roads_before\": roads_before,\n",
    "        \"roads_after\": roads_after,\n",
    "\n",
    "        \"deg2_removed\": g_deg2_removed,\n",
    "        \"deg2_remain\":  g_deg2_remain,\n",
    "\n",
    "        \"round_members\":   g_round_members,\n",
    "        \"round_centroids\": g_round_centroids,\n",
    "\n",
    "        \"snap_members\":    g_snap_members,\n",
    "        \"snap_centroids\":  g_snap_centroids,\n",
    "\n",
    "        \"noding_created\":   g_noded_created,\n",
    "        \"noding_remain\":    g_noded_remain,\n",
    "        \"noding_junctions\": g_noded_junctions,\n",
    "    }\n",
    "    summary = {k: len(v) for k, v in layers.items() if isinstance(v, gpd.GeoDataFrame)}\n",
    "    return layers, summary\n",
    "\n",
    "# ----------------------------\n",
    "# folium map helper\n",
    "# ----------------------------\n",
    "def folium_map_before_after_with_nodes(\n",
    "    layers: dict,\n",
    "    region_poly=None,\n",
    "    center_latlon=None,\n",
    "    zoom_start=6,\n",
    "    out_html=\"cleaning_nodes_before_after.html\",\n",
    "):\n",
    "    if center_latlon is None:\n",
    "        b = layers[\"roads_before\"].to_crs(4326).total_bounds\n",
    "        center_latlon = ((b[1]+b[3])/2, (b[0]+b[2])/2)\n",
    "\n",
    "    def add_geojson(parent, gdf_4326, name, color, weight=2, opacity=0.65):\n",
    "        if gdf_4326 is None or len(gdf_4326) == 0:\n",
    "            return\n",
    "        folium.GeoJson(\n",
    "            gdf_4326.__geo_interface__,\n",
    "            name=name,\n",
    "            style_function=lambda feat: {\"color\": color, \"weight\": weight, \"opacity\": opacity},\n",
    "        ).add_to(parent)\n",
    "\n",
    "    def add_points(parent, gdf_4326, name, color, radius=3):\n",
    "        if gdf_4326 is None or len(gdf_4326) == 0:\n",
    "            return\n",
    "        folium.GeoJson(\n",
    "            gdf_4326.__geo_interface__,\n",
    "            name=name,\n",
    "            marker=folium.CircleMarker(radius=radius, fill=True, color=color, fill_opacity=0.9),\n",
    "        ).add_to(parent)\n",
    "\n",
    "    m = folium.Map(location=[center_latlon[0], center_latlon[1]], zoom_start=zoom_start, tiles=\"CartoDB positron\")\n",
    "\n",
    "    fg_before = folium.FeatureGroup(name=\"ROADS — ORIGINAL (before cleaning)\", show=True)\n",
    "    fg_after  = folium.FeatureGroup(name=\"ROADS — CLEANED (after cleaning)\", show=False)\n",
    "    m.add_child(fg_before); m.add_child(fg_after)\n",
    "\n",
    "    add_geojson(fg_before, layers[\"roads_before\"].to_crs(4326), \"roads_before\", color=\"#1f77b4\", weight=2, opacity=0.65)\n",
    "    add_geojson(fg_after,  layers[\"roads_after\"].to_crs(4326),  \"roads_after\",  color=\"#d62728\", weight=2, opacity=0.65)\n",
    "\n",
    "    if region_poly is not None:\n",
    "        reg = gpd.GeoDataFrame(geometry=[region_poly], crs=\"EPSG:3395\").to_crs(4326)\n",
    "        add_geojson(m, reg, \"Region boundary\", color=\"#444444\", weight=2, opacity=0.55)\n",
    "        b = reg.total_bounds\n",
    "        m.fit_bounds([[b[1], b[0]], [b[3], b[2]]])\n",
    "\n",
    "    # node layers\n",
    "    add_points(m, layers[\"deg2_removed\"].to_crs(4326), \"Nodes REMOVED: degree-2\", color=\"#ff00ff\", radius=3)\n",
    "    add_points(m, layers[\"deg2_remain\"].to_crs(4326),  \"Nodes REMAIN: after degree-2 (H1)\", color=\"#00cc96\", radius=2)\n",
    "\n",
    "    add_points(m, layers[\"round_members\"].to_crs(4326),   \"Nodes MODIFIED: roundabout members (before)\", color=\"#ff7f0e\", radius=4)\n",
    "    add_points(m, layers[\"round_centroids\"].to_crs(4326), \"Nodes REMAIN: roundabout centroids (after)\",  color=\"#2ca02c\", radius=6)\n",
    "\n",
    "    add_points(m, layers[\"snap_members\"].to_crs(4326),   \"Nodes MODIFIED: snap members (before)\", color=\"#9467bd\", radius=4)\n",
    "    add_points(m, layers[\"snap_centroids\"].to_crs(4326), \"Nodes REMAIN: snap centroids (after)\",  color=\"#17becf\", radius=6)\n",
    "\n",
    "    add_points(m, layers[\"noding_created\"].to_crs(4326),  \"Nodes CREATED: by noding\", color=\"#e377c2\", radius=3)\n",
    "    add_points(m, layers[\"noding_remain\"].to_crs(4326),   \"Nodes REMAIN: endpoints after noding\", color=\"#7f7f7f\", radius=2)\n",
    "    add_points(m, layers[\"noding_junctions\"].to_crs(4326),\"Nodes REMAIN: junction endpoints after noding (deg>=3)\", color=\"#bcbd22\", radius=3)\n",
    "\n",
    "    folium.LayerControl(collapsed=False).add_to(m)\n",
    "    m.save(out_html)\n",
    "    return out_html\n",
    "\n",
    "layers_all, summary_all = build_cleaning_layers(\n",
    "    gdf_lines_3395=gdf_lines,\n",
    "    edges_noded_3395=edges_noded,\n",
    "    G_detailed=G,\n",
    "    H1_deg2_removed=H1,\n",
    "    info_roundabout=info_roundabout,  # or None\n",
    "    region_poly=None,                 # whole dataset\n",
    "    snap_radius_m=30.0,\n",
    "    min_junction_deg=3,\n",
    "    endpoint_prec_m=0.01,\n",
    "    max_points_removed=1000000,\n",
    "    max_points_remain=25000,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "print(summary_all)\n",
    "\n",
    "out_html = folium_map_before_after_with_nodes(\n",
    "    layers_all,\n",
    "    region_poly=None,\n",
    "    center_latlon=None,\n",
    "    zoom_start=5,\n",
    "    out_html=\"full_cleaning_nodes_before_after.html\"\n",
    ")\n",
    "\n",
    "print(\"Saved:\", out_html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2554b581",
   "metadata": {},
   "source": [
    "# 2. Weighted Network Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb0641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned network: deg-2 removed + roundabout merge -> 9,458 nodes, 12,680 edges\n",
      "edges_clean_3395 created: 12680\n",
      "Slope transferred. NaN slope fraction: 0.0\n",
      "             dist_m     slope_deg   passability  speed_kmh_eff      time_min\n",
      "count  1.268000e+04  12680.000000  12680.000000   12680.000000  1.268000e+04\n",
      "mean   2.661819e+04      2.827660      0.924007       1.848015  8.770041e+02\n",
      "std    2.971207e+04      2.234975      0.136214       0.272428  9.834805e+02\n",
      "min    5.820766e-11      0.000000      0.100000       0.200000  1.746230e-12\n",
      "25%    6.719448e+03      1.343194      0.805627       1.611255  2.223910e+02\n",
      "50%    1.819454e+04      2.118602      1.000000       2.000000  6.003567e+02\n",
      "75%    3.709574e+04      3.533673      1.000000       2.000000  1.211359e+03\n",
      "max    4.481473e+05     33.574162      1.000000       2.000000  1.344442e+04\n",
      "Time-weighted graph (EPSG:3395): 9,458 nodes, 12,680 edges\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import networkx as nx\n",
    "from shapely.geometry import Point, LineString\n",
    "from shapely.strtree import STRtree\n",
    "\n",
    "# ============================================================\n",
    "# 0) Preconditions / sanity\n",
    "# ============================================================\n",
    "need = [\"gdf_lines\", \"edges_noded\", \"edge_geoms1\", \"H1\", \"G\"]\n",
    "missing = [k for k in need if k not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing objects: {missing}. Run the cleaning/build steps first.\")\n",
    "\n",
    "if gdf_lines.crs is None or str(gdf_lines.crs).upper() != \"EPSG:3395\":\n",
    "    raise RuntimeError(f\"gdf_lines must be EPSG:3395. Found: {gdf_lines.crs}\")\n",
    "if \"Avg_Slope\" not in gdf_lines.columns:\n",
    "    raise RuntimeError(\"gdf_lines must contain Avg_Slope (degrees).\")\n",
    "\n",
    "# Keep only LineStrings\n",
    "gdf_src = gdf_lines[gdf_lines.geometry.notna() & ~gdf_lines.geometry.is_empty].copy()\n",
    "gdf_src = gdf_src[gdf_src.geometry.geom_type == \"LineString\"].copy().reset_index(drop=True)\n",
    "gdf_src[\"slope_deg_src\"] = pd.to_numeric(gdf_src[\"Avg_Slope\"], errors=\"coerce\").astype(float)\n",
    "\n",
    "# ============================================================\n",
    "# 1) Build geometry-preserving cleaned edges (deg-2 removed + optional roundabout merge)\n",
    "# ============================================================\n",
    "def undirected_key(a, b):\n",
    "    return (a, b) if a <= b else (b, a)\n",
    "\n",
    "def merge_edge_geoms_with_mapping(edge_geoms, mapping):\n",
    "    \"\"\"\n",
    "    Apply node mapping (old node -> new centroid node) to polyline edges.\n",
    "    Preserves the polyline shape by only moving first/last coordinate to the mapped node.\n",
    "    If multiple edges collapse to same undirected (u2,v2), keeps the shortest.\n",
    "    \"\"\"\n",
    "    best = {}\n",
    "    for e in edge_geoms:\n",
    "        u, v = e[\"u\"], e[\"v\"]\n",
    "        u2 = mapping.get(u, u)\n",
    "        v2 = mapping.get(v, v)\n",
    "        if u2 == v2:\n",
    "            continue\n",
    "\n",
    "        coords = list(e[\"geometry\"].coords)\n",
    "        coords[0] = u2\n",
    "        coords[-1] = v2\n",
    "        geom2 = LineString(coords)\n",
    "\n",
    "        k = undirected_key(u2, v2)\n",
    "        dist = float(geom2.length)\n",
    "\n",
    "        if (k not in best) or (dist < best[k][\"dist_m\"]):\n",
    "            best[k] = {\n",
    "                \"u\": k[0],\n",
    "                \"v\": k[1],\n",
    "                \"weight\": dist,\n",
    "                \"dist_m\": dist,\n",
    "                \"geometry\": geom2,\n",
    "            }\n",
    "    return list(best.values())\n",
    "\n",
    "def graph_from_edge_geoms(edge_geoms):\n",
    "    G = nx.Graph()\n",
    "    for e in edge_geoms:\n",
    "        u, v = e[\"u\"], e[\"v\"]\n",
    "        if u == v:\n",
    "            continue\n",
    "        w = float(e.get(\"weight\", e[\"geometry\"].length))\n",
    "        if G.has_edge(u, v):\n",
    "            G[u][v][\"weight\"] = min(float(G[u][v][\"weight\"]), w)\n",
    "        else:\n",
    "            G.add_edge(u, v, weight=w)\n",
    "    return G\n",
    "\n",
    "# Decide whether to include roundabout merge.\n",
    "# If you computed mapping via merge_close_intersections_dbscan(H1,...), use it.\n",
    "USE_ROUNDABOUT_MERGE = (\"mapping\" in globals())\n",
    "\n",
    "if USE_ROUNDABOUT_MERGE:\n",
    "    edge_geoms2 = merge_edge_geoms_with_mapping(edge_geoms1, mapping)\n",
    "    G2 = graph_from_edge_geoms(edge_geoms2)\n",
    "    H_clean, edge_geoms_clean = simplify_degree2_until_stable(G2)\n",
    "    print(f\"Cleaned network: deg-2 removed + roundabout merge -> {H_clean.number_of_nodes():,} nodes, {H_clean.number_of_edges():,} edges\")\n",
    "else:\n",
    "    edge_geoms_clean = edge_geoms1\n",
    "    H_clean = H1\n",
    "    print(f\"Cleaned network: deg-2 removed only -> {H_clean.number_of_nodes():,} nodes, {H_clean.number_of_edges():,} edges\")\n",
    "\n",
    "# Turn cleaned edge_geoms into GeoDataFrame (this is what you were missing)\n",
    "edges_clean_3395 = gpd.GeoDataFrame(edge_geoms_clean, crs=\"EPSG:3395\").copy()\n",
    "edges_clean_3395 = edges_clean_3395[edges_clean_3395.geometry.notna() & ~edges_clean_3395.geometry.is_empty].copy()\n",
    "edges_clean_3395 = edges_clean_3395[edges_clean_3395.geometry.geom_type == \"LineString\"].reset_index(drop=True)\n",
    "edges_clean_3395[\"edge_id\"] = np.arange(1, len(edges_clean_3395) + 1, dtype=\"int64\")\n",
    "edges_clean_3395[\"dist_m\"] = edges_clean_3395.geometry.length.astype(float)\n",
    "edges_clean_3395[\"dist_km\"] = edges_clean_3395[\"dist_m\"] / 1000.0\n",
    "\n",
    "print(\"edges_clean_3395 created:\", len(edges_clean_3395))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2) Transfer slope from original segments -> cleaned edges (length-weighted overlap)\n",
    "# ============================================================\n",
    "def slope_transfer_length_weighted_deg(edges_gdf, src_segments_gdf, buffer_m=5.0, min_intersection_m=1.0):\n",
    "    \"\"\"\n",
    "    For each cleaned edge:\n",
    "      - query source segments within buffer_m\n",
    "      - compute intersection length with each candidate\n",
    "      - slope_deg = weighted mean by intersection length\n",
    "      - fallback: nearest candidate in buffer (if no overlap)\n",
    "    \"\"\"\n",
    "    edges = edges_gdf.copy()\n",
    "\n",
    "    src_geoms = list(src_segments_gdf.geometry)\n",
    "    src_slopes = src_segments_gdf[\"slope_deg_src\"].to_numpy(dtype=float)\n",
    "    tree = STRtree(src_geoms)\n",
    "\n",
    "    # For Shapely<2 fallback, we need WKB->idx map\n",
    "    wkb_to_idx = {g.wkb: i for i, g in enumerate(src_geoms)}\n",
    "\n",
    "    out = np.full(len(edges), np.nan, dtype=float)\n",
    "\n",
    "    for i, geom in enumerate(edges.geometry):\n",
    "        q = geom.buffer(buffer_m)\n",
    "        res = tree.query(q)\n",
    "\n",
    "        if len(res) == 0:\n",
    "            continue\n",
    "\n",
    "        # Shapely 2.x: indices, else geometries\n",
    "        if isinstance(res[0], (int, np.integer)):\n",
    "            cand_idx = [int(x) for x in res]\n",
    "        else:\n",
    "            cand_idx = [wkb_to_idx.get(g.wkb) for g in res]\n",
    "            cand_idx = [j for j in cand_idx if j is not None]\n",
    "\n",
    "        num = 0.0\n",
    "        den = 0.0\n",
    "        best_d = np.inf\n",
    "        best_s = np.nan\n",
    "\n",
    "        for j in cand_idx:\n",
    "            seg = src_geoms[j]\n",
    "            s = src_slopes[j]\n",
    "            if np.isnan(s):\n",
    "                continue\n",
    "\n",
    "            d = geom.distance(seg)\n",
    "            if d < best_d:\n",
    "                best_d = d\n",
    "                best_s = s\n",
    "\n",
    "            inter = geom.intersection(seg)\n",
    "            if inter.is_empty:\n",
    "                continue\n",
    "            L = float(inter.length)\n",
    "            if L < min_intersection_m:\n",
    "                continue\n",
    "            num += L * float(s)\n",
    "            den += L\n",
    "\n",
    "        if den > 0:\n",
    "            out[i] = num / den\n",
    "        else:\n",
    "            out[i] = float(best_s) if not np.isnan(best_s) else np.nan\n",
    "\n",
    "    edges[\"slope_deg\"] = out\n",
    "    return edges\n",
    "\n",
    "edges_w = slope_transfer_length_weighted_deg(edges_clean_3395, gdf_src, buffer_m=5.0, min_intersection_m=1.0)\n",
    "print(\"Slope transferred. NaN slope fraction:\", float(edges_w[\"slope_deg\"].isna().mean()))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3) Passability + time weights (Ox cart 2 km/h) using Tobler ratio; threshold 6%\n",
    "# ============================================================\n",
    "def tobler_speed_kmh(grad):\n",
    "    # Tobler hiking function (km/h)\n",
    "    return float(6.0 * np.exp(-3.5 * abs(grad + 0.05)))\n",
    "\n",
    "def passability_multiplier_from_slope_deg(slope_deg, threshold_pct=6.0):\n",
    "    \"\"\"\n",
    "    Isotropic passability:\n",
    "      - convert degrees -> gradient = tan(theta)\n",
    "      - if grad <= 0.06 => multiplier 1.0\n",
    "      - else multiplier = v(grad)/v(0), clipped to [0.1, 1.0]\n",
    "    \"\"\"\n",
    "    if slope_deg is None or np.isnan(slope_deg):\n",
    "        return 1.0\n",
    "    grad = abs(np.tan(np.deg2rad(float(slope_deg))))\n",
    "    if grad <= (threshold_pct / 100.0):\n",
    "        return 1.0\n",
    "    v0 = tobler_speed_kmh(0.0)\n",
    "    vs = tobler_speed_kmh(grad)\n",
    "    mult = (vs / v0) if v0 > 0 else 1.0\n",
    "    return float(np.clip(mult, 0.1, 1.0))\n",
    "\n",
    "def add_time_weights(edges_gdf, base_speed_kmh=2.0, threshold_pct=6.0):\n",
    "    edges = edges_gdf.copy()\n",
    "    edges[\"passability\"] = edges[\"slope_deg\"].apply(lambda d: passability_multiplier_from_slope_deg(d, threshold_pct))\n",
    "    edges[\"speed_kmh_eff\"] = (base_speed_kmh * edges[\"passability\"]).astype(float)\n",
    "    edges[\"time_h\"] = (edges[\"dist_km\"] / edges[\"speed_kmh_eff\"]).astype(float)\n",
    "    edges[\"time_s\"] = (edges[\"time_h\"] * 3600.0).astype(float)\n",
    "    edges[\"time_min\"] = (edges[\"time_s\"] / 60.0).astype(float)\n",
    "    return edges\n",
    "\n",
    "edges_w = add_time_weights(edges_w, base_speed_kmh=2.0, threshold_pct=6.0)\n",
    "print(edges_w[[\"dist_m\",\"slope_deg\",\"passability\",\"speed_kmh_eff\",\"time_min\"]].describe())\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4) Build time-weighted NetworkX graph (weight = time_s)\n",
    "# ============================================================\n",
    "def build_time_weighted_graph(edges_gdf):\n",
    "    Gt = nx.Graph()\n",
    "    for row in edges_gdf.itertuples(index=False):\n",
    "        geom = row.geometry\n",
    "        coords = list(geom.coords)\n",
    "        if len(coords) < 2:\n",
    "            continue\n",
    "        u = (float(coords[0][0]), float(coords[0][1]))\n",
    "        v = (float(coords[-1][0]), float(coords[-1][1]))\n",
    "        if u == v:\n",
    "            continue\n",
    "\n",
    "        time_s = float(getattr(row, \"time_s\"))\n",
    "        dist_m = float(getattr(row, \"dist_m\"))\n",
    "        slope_deg = float(getattr(row, \"slope_deg\")) if not np.isnan(getattr(row, \"slope_deg\")) else np.nan\n",
    "        passability = float(getattr(row, \"passability\"))\n",
    "        speed_kmh_eff = float(getattr(row, \"speed_kmh_eff\"))\n",
    "\n",
    "        # keep fastest if duplicates exist\n",
    "        if Gt.has_edge(u, v):\n",
    "            if time_s < Gt[u][v][\"weight_time_s\"]:\n",
    "                Gt[u][v].update(\n",
    "                    weight_time_s=time_s,\n",
    "                    dist_m=dist_m,\n",
    "                    slope_deg=slope_deg,\n",
    "                    passability=passability,\n",
    "                    speed_kmh_eff=speed_kmh_eff,\n",
    "                    geometry=geom,\n",
    "                )\n",
    "        else:\n",
    "            Gt.add_edge(\n",
    "                u, v,\n",
    "                weight_time_s=time_s,\n",
    "                dist_m=dist_m,\n",
    "                slope_deg=slope_deg,\n",
    "                passability=passability,\n",
    "                speed_kmh_eff=speed_kmh_eff,\n",
    "                geometry=geom,\n",
    "            )\n",
    "    return Gt\n",
    "\n",
    "Gt_3395 = build_time_weighted_graph(edges_w)\n",
    "print(f\"Time-weighted graph (EPSG:3395): {Gt_3395.number_of_nodes():,} nodes, {Gt_3395.number_of_edges():,} edges\")\n",
    "\n",
    "Gu = nx.Graph()\n",
    "for u, v, d in Gt_3395.edges(data=True):\n",
    "    w = d.get(\"weight\", 1.0)\n",
    "    if Gu.has_edge(u, v):\n",
    "        Gu[u][v][\"weight\"] = min(Gu[u][v][\"weight\"], w)\n",
    "    else:\n",
    "        Gu.add_edge(u, v, **d)\n",
    "\n",
    "Gt_3395 = Gu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a38875",
   "metadata": {},
   "source": [
    "# Check: Nodes and Edges consitency across the two networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520af251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1 nodes/edges: 9624 12874\n",
      "deg2_removed_calc: 988210\n",
      "H_clean nodes/edges: 9458 12680\n",
      "edges_clean_3395 rows: 12680\n"
     ]
    }
   ],
   "source": [
    "# print(\"H1 nodes/edges:\", H1.number_of_nodes(), H1.number_of_edges())\n",
    "# # expect ~9624 nodes, ~12874 edges (your earlier numbers)\n",
    "# deg2_removed_calc = sum(1 for n,d in dict(G.degree()).items() if d==2 and n not in set(H1.nodes()))\n",
    "# print(\"deg2_removed_calc:\", deg2_removed_calc)\n",
    "# print(\"H_clean nodes/edges:\", H_clean.number_of_nodes(), H_clean.number_of_edges())\n",
    "# print(\"edges_clean_3395 rows:\", len(edges_clean_3395))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f53057e",
   "metadata": {},
   "source": [
    "# 3. Weighted Network Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d60dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodes                              : 9458\n",
      "edges                              : 12680\n",
      "avg_degree                         : 2.681327976316346\n",
      "avg_weighted_degree                : 141092.14290345868\n",
      "connected_components               : 218\n",
      "largest_component_nodes            : 4784\n",
      "largest_component_edges            : 7017\n",
      "largest_component_fraction_of_nodes: 0.5058151829139353\n"
     ]
    }
   ],
   "source": [
    "def graph_stats(G: nx.Graph, weight: str = \"weight\"):\n",
    "    \"\"\"\n",
    "    Compute basic statistics for an (un)weighted NetworkX graph.\n",
    "\n",
    "    Assumptions:\n",
    "      - Undirected graph (nx.Graph). If it's a DiGraph, see note below.\n",
    "      - Edge attribute `weight` exists for weighted degree; if missing, treated as 1.\n",
    "    \"\"\"\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "\n",
    "    # Average degree: 2m / n for undirected (guard n=0)\n",
    "    avg_deg = (2.0 * m / n) if n else 0.0\n",
    "\n",
    "    # Weighted degree: sum of incident weights per node (strength)\n",
    "    # If an edge has no 'weight', NetworkX treats it as 1 for degree(weight=...)\n",
    "    wdeg = dict(G.degree(weight='weight_time_s'))\n",
    "    avg_wdeg = (float(np.mean(list(wdeg.values()))) if n else 0.0)\n",
    "\n",
    "    # Connected components\n",
    "    comps = list(nx.connected_components(G))\n",
    "    num_cc = len(comps)\n",
    "    largest_cc_size = max((len(c) for c in comps), default=0)\n",
    "\n",
    "    # Optional: edges in largest component + fraction of nodes in it\n",
    "    if largest_cc_size > 0:\n",
    "        largest_nodes = max(comps, key=len)\n",
    "        G_lcc = G.subgraph(largest_nodes)\n",
    "        lcc_edges = G_lcc.number_of_edges()\n",
    "        lcc_frac = largest_cc_size / n if n else 0.0\n",
    "    else:\n",
    "        lcc_edges = 0\n",
    "        lcc_frac = 0.0\n",
    "\n",
    "    out = {\n",
    "        \"nodes\": n,\n",
    "        \"edges\": m,\n",
    "        \"avg_degree\": avg_deg,\n",
    "        \"avg_weighted_degree\": avg_wdeg,\n",
    "        \"connected_components\": num_cc,\n",
    "        \"largest_component_nodes\": largest_cc_size,\n",
    "        \"largest_component_edges\": lcc_edges,\n",
    "        \"largest_component_fraction_of_nodes\": lcc_frac,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "stats = graph_stats(Gt_3395, weight=\"weight\")\n",
    "for k, v in stats.items():\n",
    "    print(f\"{k:35s}: {v}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
